<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="keywords" content="ML, Elastic Stack, natural language processing, inference">
<title>Benchmark information | Machine Learning in the Elastic Stack [8.11] | Elastic</title>
<meta class="elastic" name="content" content="Benchmark information | Machine Learning in the Elastic Stack [8.11]">

<link rel="home" href="index.html" title="Machine Learning in the Elastic Stack [8.11]"/>
<link rel="up" href="ml-nlp-elser.html" title="ELSER – Elastic Learned Sparse EncodeR"/>
<link rel="prev" href="ml-nlp-elser.html" title="ELSER – Elastic Learned Sparse EncodeR"/>
<link rel="next" href="ml-nlp-model-ref.html" title="Compatible third party NLP models"/>
<meta class="elastic" name="product_version" content="8.11"/>
<meta class="elastic" name="product_name" content="Machine Learning"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Elastic Stack/Machine Learning/8.11"/>
<meta name="DC.subject" content="Machine Learning"/>
<meta name="DC.identifier" content="8.11"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
You are looking at preliminary documentation for a future release.
Not what you want? See the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Machine Learning in the Elastic Stack [8.11]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="ml-nlp.html">Natural language processing</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="ml-nlp-elser.html">ELSER – Elastic Learned Sparse EncodeR</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="ml-nlp-elser.html">« ELSER – Elastic Learned Sparse EncodeR</a>
</span>
<span class="next">
<a href="ml-nlp-model-ref.html">Compatible third party NLP models »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="elser-benchamrks"></a>Benchmark information<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.11/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h2>
</div></div></div>
<p>The following sections provide information about how ELSER performs on different
hardwares and compares the model performance to Elasticsearch BM25 and other strong
baselines such as Splade or OpenAI.</p>
<h4><a id="elser-qualitative-benchmarks"></a>Qualitative benchmarks<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.11/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h4>
<p>The metric that is used to evaluate ELSER&#8217;s ranking ability is the Normalized
Discounted Cumulative Gain (NDCG) which can handle multiple relevant documents
and fine-grained document ratings. The metric is applied to a fixed-sized list
of retrieved documents which, in this case, is the top 10 documents (NDCG@10).</p>
<p>The table below shows the performance of ELSER v2 compared to ELSER v1. ELSER v2
has 10 wins, 1 draw, 1 loss and an average improvement in NDCG@10 of 2.5%.</p>
<div class="imageblock text-center">
<div class="content">
<img src="images/ml-nlp-elser-v1-v2.png" alt="ELSER v2 benchmarks compared to ELSER v1">
</div>
</div>
<p><em>NDCG@10 for BEIR data sets for ELSER v2 and ELSER v1  - higher values are better)</em></p>
<p>The next table shows the performance of ELSER v1 compared to Elasticsearch BM25 with an
English analyzer broken down by the 12 data sets used for the evaluation. ELSER
v1 has 10 wins, 1 draw, 1 loss and an average improvement in NDCG@10 of 17%.</p>
<div class="imageblock text-center">
<div class="content">
<img src="images/ml-nlp-elser-ndcg10-beir.png" alt="ELSER v1 benchmarks">
</div>
</div>
<p><em>NDCG@10 for BEIR data sets for BM25 and ELSER v1  - higher values are better)</em></p>
<p>The following table compares the average performance of ELSER v1 to some other
strong baselines. The OpenAI results are separated out because they use a
different subset of the BEIR suite.</p>
<div class="imageblock text-center">
<div class="content">
<img src="images/ml-nlp-elser-average-ndcg.png" alt="ELSER v1 average performance compared to other baselines">
</div>
</div>
<p><em>Average NDCG@10 for BEIR data sets vs. various high quality baselines (higher</em>
<em>is better). OpenAI chose a different subset, ELSER v1 results on this set</em>
<em>reported separately.</em></p>
<p>To read more about the evaluation details, refer to
<a href="/blog/may-2023-launch-information-retrieval-elasticsearch-ai-model" class="ulink" target="_top">this blog post</a>.</p>
<h4><a id="elser-hw-benchmarks"></a>Hardware benchmarks<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.11/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h4>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>While the goal is to create a model that is as performant as
possible, retrieval accuracy always take precedence over speed, this is one of
the design principles of ELSER. Consult with the tables below to learn more
about the expected model performance. The values refer to operations performed
on two data sets and different hardware configurations. Your data set has an
impact on the model performance. Run tests on your own data to have a more
realistic view on the model performance for your use case.</p>
</div>
</div>
<h5><a id="_elser_v1"></a>ELSER v1<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.11/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h5>
<p>Two data sets were utilized to evaluate the performance of ELSER v1 in different
hardware configurations: <code class="literal">msmarco-long-light</code> and <code class="literal">arguana</code>.</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<tbody>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>Data set</strong></span></p></td>
<td align="center" valign="top"><p><span class="strong strong"><strong>Data set size</strong></span></p></td>
<td align="center" valign="top"><p><span class="strong strong"><strong>Average count of tokens / query</strong></span></p></td>
<td align="center" valign="top"><p><span class="strong strong"><strong>Average count of tokens / document</strong></span></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">msmarco-long-light</code></p></td>
<td align="center" valign="top"><p>37367 documents</p></td>
<td align="center" valign="top"><p>9</p></td>
<td align="center" valign="top"><p>1640</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">arguana</code></p></td>
<td align="center" valign="top"><p>8674 documents</p></td>
<td align="center" valign="top"><p>238</p></td>
<td align="center" valign="top"><p>202</p></td>
</tr>
</tbody>
</table>
</div>
<p>The <code class="literal">msmarco-long-light</code> data set contains long documents with an average of
over 512 tokens, which provides insights into the performance implications
of indexing and inference time for long documents. This is a subset of the
"msmarco" dataset specifically designed for document retrieval (it shouldn&#8217;t be
confused with the "msmarco" dataset used for passage retrieval, which primarily
consists of shorter spans of text).</p>
<p>The <code class="literal">arguana</code> data set is a <a href="https://github.com/beir-cellar/beir" class="ulink" target="_top">BEIR</a> data set.
It consists of long queries with an average of 200 tokens per query. It can
represent an upper limit for query slowness.</p>
<p>The table below present benchmarking results for ELSER using various hardware
configurations.</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
<col class="col_5"/>
<col class="col_6"/>
<col class="col_7"/>
<col class="col_8"/>
</colgroup>
<tbody>
<tr>
<td align="left" valign="top"><p></p></td>
<td align="center" colspan="3" valign="top"><p><code class="literal">msmarco-long-light</code></p></td>
<td align="center" colspan="3" valign="top"><p><code class="literal">arguana</code></p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p></p></td>
<td align="center" valign="middle"><p>inference</p></td>
<td align="center" valign="middle"><p>indexing</p></td>
<td align="center" valign="middle"><p>query latency</p></td>
<td align="center" valign="middle"><p>inference</p></td>
<td align="center" valign="middle"><p>indexing</p></td>
<td align="center" valign="middle"><p>query latency</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>ML node 4GB - 2 vCPUs (1 allocation * 1 thread)</strong></span></p></td>
<td align="center" valign="middle"><p>581   ms/call</p></td>
<td align="center" valign="middle"><p>1.7   doc/sec</p></td>
<td align="center" valign="middle"><p>713   ms/query</p></td>
<td align="center" valign="middle"><p>1200   ms/call</p></td>
<td align="center" valign="middle"><p>0.8   doc/sec</p></td>
<td align="center" valign="middle"><p>169   ms/query</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>ML node 16GB - 8 vCPUs (7 allocation * 1 thread)</strong></span></p></td>
<td align="center" valign="middle"><p>568   ms/call</p></td>
<td align="center" valign="middle"><p>12    doc/sec</p></td>
<td align="center" valign="middle"><p>689   ms/query</p></td>
<td align="center" valign="middle"><p>1280   ms/call</p></td>
<td align="center" valign="middle"><p>5.4   doc/sec</p></td>
<td align="center" valign="middle"><p>159   ms/query</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>ML node 16GB - 8 vCPUs (1 allocation * 8 thread)</strong></span></p></td>
<td align="center" valign="middle"><p>102   ms/call</p></td>
<td align="center" valign="middle"><p>9.7   doc/sec</p></td>
<td align="center" valign="middle"><p>164   ms/query</p></td>
<td align="center" valign="middle"><p>220    ms/call</p></td>
<td align="center" valign="middle"><p>4.5   doc/sec</p></td>
<td align="center" valign="middle"><p>40    ms/query</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>ML node 32 GB - 16 vCPUs (15 allocation * 1 thread)</strong></span></p></td>
<td align="center" valign="middle"><p>565   ms/call</p></td>
<td align="center" valign="middle"><p>25.2  doc/sec</p></td>
<td align="center" valign="middle"><p>608   ms/query</p></td>
<td align="center" valign="middle"><p>1260   ms/call</p></td>
<td align="center" valign="middle"><p>11.4  doc/sec</p></td>
<td align="center" valign="middle"><p>138   ms/query</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="navfooter">
<span class="prev">
<a href="ml-nlp-elser.html">« ELSER – Elastic Learned Sparse EncodeR</a>
</span>
<span class="next">
<a href="ml-nlp-model-ref.html">Compatible third party NLP models »</a>
</span>
</div>
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains('stemblock')) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>
