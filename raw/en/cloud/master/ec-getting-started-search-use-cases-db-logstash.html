<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Ingest data from a relational database into Elastic Cloud | Elasticsearch Service Documentation [master] | Elastic</title>
<link rel="home" href="index.html" title="Elasticsearch Service Documentation [master]"/>
<link rel="up" href="ec-getting-started.html" title="Getting started"/>
<link rel="prev" href="ec-getting-started-search-use-case.html" title="Ingest data with Node.js on Elastic Cloud"/>
<link rel="next" href="ec-api-console.html" title="Access the Elasticsearch API console"/>
<meta name="DC.type" content="Learn/Docs/Cloud/Reference/master"/>
<meta name="DC.subject" content="Elastic Cloud"/>
<meta name="DC.identifier" content="master"/>
</head>
<body><div class="page_header">
You are looking at preliminary documentation for a future release.
Not what you want? See the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Elasticsearch Service Documentation [master]</a></span>
»
<span class="breadcrumb-link"><a href="ec-getting-started.html">Getting started</a></span>
»
<span class="breadcrumb-node">Ingest data from a relational database into Elastic Cloud</span>
</div>
<div class="navheader">
<span class="prev">
<a href="ec-getting-started-search-use-case.html">« Ingest data with Node.js on Elastic Cloud</a>
</span>
<span class="next">
<a href="ec-api-console.html">Access the Elasticsearch API console »</a>
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="ec-getting-started-search-use-cases-db-logstash"></a>Ingest data from a relational database into Elastic Cloud<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/master/docs/saas/ec-getting-started-use-cases-db-logstash.asciidoc">edit</a></h2>
</div></div></div>
<p>This guide explains how to ingest data from a relational database into the Elastic Cloud through <a href="/guide/en/logstash/7.12/introduction.html" class="ulink" target="_top">Logstash</a> and the <a href="/guide/en/logstash/7.12/introduction.html" class="ulink" target="_top">JDBC input plugin</a>. It demonstrates how Logstash can be used to efficiently copy records and to receive updates from a relational database into Elasticsearch.</p>
<p>The code and methods presented here have been tested with MySQL. They should work with other relational databases.</p>
<p>The Java Database Connectivity (JDBC) input plugin enables you to pull in data from many popular relational databases including MySQL and Postgres. Conceptually, the Logstash JDBC input plugin runs a loop that periodically polls the relational database for records that were inserted or modified since the last iteration of this loop.</p>
<p>This document presents how to:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="ec-getting-started-search-use-cases-db-logstash.html#ec-db-logstash-instance" title="Get Logstash and Elasticsearch Service">Get Logstash and Elasticsearch Service</a>
</li>
<li class="listitem">
<a class="xref" href="ec-getting-started-search-use-cases-db-logstash.html#ec-db-logstash-driver" title="Get the MySQL JDBC driver">Get the MySQL JDBC driver</a>
</li>
<li class="listitem">
<a class="xref" href="ec-getting-started-search-use-cases-db-logstash.html#ec-db-logstash-database" title="Prepare a source MySQL database">Prepare a source MySQL database</a>
</li>
<li class="listitem">
<a class="xref" href="ec-getting-started-search-use-cases-db-logstash.html#ec-db-logstash-pipeline" title="Configure a Logstash pipeline with the JDBC plugin">Configure a Logstash pipeline with the JDBC plugin</a>
</li>
<li class="listitem">
<a class="xref" href="ec-getting-started-search-use-cases-db-logstash.html#ec-db-logstash-output" title="Output to Elasticsearch">Output to Elasticsearch</a>
</li>
</ol>
</div>
<p><em>Time required: 2 hours</em></p>
<h3><a id="ec_prerequisites_2"></a>Prerequisites<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/master/docs/saas/ec-getting-started-use-cases-db-logstash.asciidoc">edit</a></h3>
<p>For this tutorial you will need a source MySQL instance for Logstash to read from. A free version of MySQL is available from the <a href="https://dev.mysql.com/downloads/mysql/" class="ulink" target="_top">MySQL Community Downloads</a> site.</p>
<h3><a id="ec-db-logstash-instance"></a>Get Logstash and Elasticsearch Service<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/master/docs/saas/ec-getting-started-use-cases-db-logstash.asciidoc">edit</a></h3>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a href="/cloud/elasticsearch-service/signup?baymax=docs-body&amp;elektra=docs" class="ulink" target="_top">Get a free trial</a>.
</li>
<li class="listitem">
Log into <a href="https://cloud.elastic.co?baymax=docs-body&amp;elektra=docs" class="ulink" target="_top">Elastic Cloud</a>.
</li>
<li class="listitem">
Click <span class="strong strong"><strong>Create deployment</strong></span>.
</li>
<li class="listitem">
Select <span class="strong strong"><strong>Elastic Stack</strong></span>, leave it at the I/O optimized default, and give your deployment a name.
</li>
<li class="listitem">
Click <span class="strong strong"><strong>Create deployment</strong></span> and save your Elastic deployment credentials. You will need these credentials later on.
</li>
<li class="listitem">
<p>You also need the Cloud ID later on, as it simplifies sending data to Elastic Cloud. Click on the deployment name from the Elastic Cloud portal or the <span class="strong strong"><strong>Deployments</strong></span> page and copy down the information under <span class="strong strong"><strong>Cloud ID</strong></span>:</p>
<p><span class="image"><img src="images/ec-cloud-id-scenarios.png" alt="A picture highlighting the Cloud ID information available for the deployment"></span></p>
<p>Prefer not to subscribe to yet another service? You can also get Elasticsearch Service through <a class="xref" href="ec-marketplaces.html" title="Subscribing from a marketplace">AWS, Azure, and GCP marketplaces</a>.</p>
</li>
<li class="listitem">
<a href="/downloads/logstash" class="ulink" target="_top">Download</a> and unpack Logstash version 7.12.0 on the local machine that hosts MySQL or another machine granted access to the MySQL machine.
</li>
</ol>
</div>
<h3><a id="ec-db-logstash-driver"></a>Get the MySQL JDBC driver<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/master/docs/saas/ec-getting-started-use-cases-db-logstash.asciidoc">edit</a></h3>
<p>The logstash-integration-jdbc plugin does not include any database connection drivers. You will need a JDBC driver for your relational database for the steps in the later section <a class="xref" href="ec-getting-started-search-use-cases-db-logstash.html#ec-db-logstash-pipeline" title="Configure a Logstash pipeline with the JDBC plugin">Configure a Logstash pipeline with the JDBC plugin</a>.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Download and unpack the JDBC driver for MySQL from <a href="https://dev.mysql.com/downloads/connector/j/" class="ulink" target="_top">https://dev.mysql.com/downloads/connector/j/</a>.
</li>
<li class="listitem">
Make a note of the driver’s location as it’s needed in the steps that follow.
</li>
</ol>
</div>
<h3><a id="ec-db-logstash-database"></a>Prepare a source MySQL database<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/master/docs/saas/ec-getting-started-use-cases-db-logstash.asciidoc">edit</a></h3>
<p>Let’s look at a simple database that from which we will import data and send it to the Elastic stack. This example uses a MySQL database with timestamped records. The timestamps enable you to easily determine what’s changed in the database since the most recent data transfer to the Elastic stack.</p>
<h4><a id="ec_consider_the_databases_structure_and_design"></a>Consider the database’s structure and design<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/master/docs/saas/ec-getting-started-use-cases-db-logstash.asciidoc">edit</a></h4>
<p>For our example, let’s create a new database ‘es_db‘ and table ‘es_table‘ as the source of our Elasticsearch data.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<p>Run the following SQL statement to generate a new MySQL database with the needed structure:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">CREATE DATABASE es_db;
USE es_db;
DROP TABLE IF EXISTS es_table;
CREATE TABLE es_table (
  id BIGINT(20) UNSIGNED NOT NULL,
  PRIMARY KEY (id),
  UNIQUE KEY unique_id (id),
  client_name VARCHAR(32) NOT NULL,
  modification_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);</pre>
</div>
<p>Let’s explore the key concepts from the above SQL snippet:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
es_table
</span>
</dt>
<dd>
Stores the data that’s sent to Elasticsearch. You can use any table name you want.
</dd>
<dt>
<span class="term">
id
</span>
</dt>
<dd>
This is the unique identifier for records. "Id" is defined as both a PRIMARY KEY and UNIQUE KEY to guarantee each "id" only appears once in the current table. This is translated to "_id" for updating or inserting the document into Elasticsearch.
</dd>
<dt>
<span class="term">
client_name
</span>
</dt>
<dd>
This is the data that we’re ultimately ingesting into the Elastic stack. For simplicity, our example only consumes a single data field into Elastic. However, you can send as many fields as is necessary.
</dd>
<dt>
<span class="term">
modification_time
</span>
</dt>
<dd>
This field’s type is TIMESTAMP. Any insertion or update of individual record updates this value. In later sections, we use this timestamp to determine what’s different since the last transfer to Elasticsearch.
</dd>
</dl>
</div>
</li>
<li class="listitem">
<p>Consider how to handle deletions and how to notify Elasticsearch about them. Often, deleting a record results in its immediate removal from the MySQL database. There’s no record of that deletion. The change isn’t detected by Logstash, so that record remains within Elasticsearch.</p>
<p>There are two possible ways to address this:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
You can consider using "soft deletes" within your source database. Essentially, a record is 1st marked for deletion through a boolean flag. Other programs that are currently using your source database would have to filter out "soft deletes" within their queries. The "soft deletes" are sent over to Elasticsearch, where they can be processed. After which, your source database and Elasticsearch must both remove these "soft deletes."
</li>
<li class="listitem">
Periodically clear the Elasticsearch version for the database and then pull everything in fresh.
</li>
</ol>
</div>
</li>
<li class="listitem">
<p>Add three records to your new database:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">INSERT INTO es_table (id, client_name) VALUES (1,"Targaryen"),(2,"Lannister"),(3,"Stark");</pre>
</div>
</li>
<li class="listitem">
<p>Verify your data with a SQL statement:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">select * from es_table;</pre>
</div>
<p>The output should look similar to the following:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">+----+-------------+---------------------+
| id | client_name | modification_time   |
+----+-------------+---------------------+
|  1 | Targaryen   | 2021-04-21 12:17:16 |
|  2 | Lannister   | 2021-04-21 12:17:16 |
|  3 | Stark       | 2021-04-21 12:17:16 |
+----+-------------+---------------------+</pre>
</div>
<p>Now, let’s go back to Logstash and configure it to ingest this data.</p>
</li>
</ol>
</div>
<h3><a id="ec-db-logstash-pipeline"></a>Configure a Logstash pipeline with the JDBC plugin<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/master/docs/saas/ec-getting-started-use-cases-db-logstash.asciidoc">edit</a></h3>
<p>Next, let’s look at the syntax for a sample Logstash input pipeline that utilizes your new JDBC plugin along with the sample MySQL database. The Logstash pipeline implements the code to ingest the previously covered MySQL database. Beyond MySQL, you can use any JDBC supported database you want for your implementation.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Go to &lt;localpath&gt;/logstash-7.12.0/. Create a new text file with a .conf file extension within this folder (i.e., jdbc.conf).
</li>
<li class="listitem">
<p>Copy and paste the following code into this new text file. This code creates a Logstash pipeline through a JDBC plugin.</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">input {
  jdbc {
    jdbc_driver_library =&gt; "&lt;driverpath&gt;/mysql-connector-java-&lt;versionNumber&gt;.jar"
    jdbc_driver_class =&gt; "com.mysql.jdbc.Driver"
    jdbc_connection_string =&gt; "jdbc:mysql://&lt;MySQL host&gt;:3306/es_db"
    jdbc_user =&gt; "&lt;myusername&gt;"
    jdbc_password =&gt; "&lt;mypassword&gt;"
    jdbc_paging_enabled =&gt; true
    tracking_column =&gt; "unix_ts_in_secs"
    use_column_value =&gt; true
    tracking_column_type =&gt; "numeric"
    schedule =&gt; "*/5 * * * * *"
    statement =&gt; "SELECT *, UNIX_TIMESTAMP(modification_time) AS unix_ts_in_secs FROM es_table WHERE (UNIX_TIMESTAMP(modification_time) &gt; :sql_last_value AND modification_time &lt; NOW()) ORDER BY modification_time ASC"
  }
}
filter {
  mutate {
    copy =&gt; { "id" =&gt; "[@metadata][_id]"}
    remove_field =&gt; ["id", "@version", "unix_ts_in_secs"]
  }
}
output {
  stdout { codec =&gt;  "rubydebug"}
}</pre>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>If you are using MariaDB (a popular open source community fork of MySQL), there are a couple of things that you need to do differently:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
In place of the MySQL JDBC driver, download and unpack the <a href="https://downloads.mariadb.org/connector-java/" class="ulink" target="_top">JDBC driver for MariaDB</a>.
</li>
<li class="listitem">
<p>Substitute the following lines in the previous code, including the <code class="literal">ANSI_QUOTES</code> snippet in the last line:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">jdbc_driver_library =&gt; "&lt;driverPath&gt;/mariadb-java-client-&lt;versionNumber&gt;.jar"
jdbc_driver_class =&gt; "org.mariadb.jdbc.Driver"
jdbc_connection_string =&gt; "jdbc:mariadb://&lt;mySQLHost&gt;:3306/es_db?sessionVariables=sql_mode=ANSI_QUOTES"</pre>
</div>
</li>
</ol>
</div>
</div>
</div>
</li>
<li class="listitem">
<p>Replace:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">&lt;driverpath&gt;</code> with the full path to your local JDBC driver .jar file. For example: <code class="literal">jdbc_driver_library =&gt; "/Users/jimmy/17jdbc-driver/mysql-connector-java-8.0.24/mysql-connector-java-8.0.24.jar"</code>
</li>
<li class="listitem">
<code class="literal">&lt;versionNumber&gt;</code> with the version of the MySQL JDBC driver that you downloaded.
</li>
<li class="listitem">
<code class="literal">&lt;MySQLhost&gt;</code> with the IP address of your MySQL host. For example, <code class="literal">jdbc_connection_string =&gt; "jdbc:mysql://127.0.0.1:3306/es_db"</code>
</li>
<li class="listitem">
<p><code class="literal">&lt;myusername&gt;</code> and <code class="literal">&lt;mypassword&gt;</code> with your MySQL credentials. The username and password must both be enclosed in quotes.</p>
<p>Following are some additional details about the Logstash pipeline code:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
jdbc_driver_library
</span>
</dt>
<dd>
This plugin does not come packaged with JDBC driver libraries. The desired jdbc driver library must be explicitly passed into the plugin using the jdbc_driver_library configuration option.
</dd>
<dt>
<span class="term">
tracking_column
</span>
</dt>
<dd>
Specifies the field "unix_ts_in_secs" (described below) that tracks the last document read by Logstash from MySQL, stored on disk in <a href="/guide/en/logstash/7.12/plugins-inputs-jdbc.html#plugins-inputs-jdbc-last_run_metadata_path" class="ulink" target="_top">logstash_jdbc_last_run</a>. This value determines the starting value for documents that Logstash requests in the next iteration of its polling loop. The value stored in .logstash_jdbc_last_run is accessible within the SELECT statement as ":sql_last_value".
</dd>
<dt>
<span class="term">
unix_ts_in_secs
</span>
</dt>
<dd>
Field that’s generated by the above SELECT statement, and which contains the "modification_time" as a standard <a href="https://en.wikipedia.org/wiki/Unix_time" class="ulink" target="_top">Unix timestamp</a> (seconds since the epoch). It’s referenced by the "tracking column" that we discussed. A Unix timestamp is used for tracking progress rather than a normal timestamp, as a normal timestamp may cause errors due to the complexity of correctly converting back and forth between UMT and the local timezone.
</dd>
<dt>
<span class="term">
sql_last_value
</span>
</dt>
<dd>
This is a <a href="/guide/en/logstash/7.12/plugins-inputs-jdbc.html#_predefined_parameters" class="ulink" target="_top">built-in parameter</a> containing the starting point of the current iteration of the Logstash polling loop, and it is referenced in the SELECT statement line of the above JDBC input configuration. This is set to the most recent value of "unix_ts_in_secs", which is read from .logstash_jdbc_last_run. This is the starting point for documents returned by the MySQL query that is executed in the Logstash polling loop. Including this variable in the query guarantees that we’re not resending data we already have within Elasticsearch.
</dd>
<dt>
<span class="term">
schedule
</span>
</dt>
<dd>
This uses cron syntax to specify how often Logstash should poll MySQL for changes. The specification of "*/5 * * * * *" tells Logstash to contact MySQL every 5 seconds.  Input from this plugin can be scheduled to run periodically according to a specific schedule. This scheduling syntax is powered by rufus-scheduler. The syntax is cron-like with some extensions specific to Rufus (e.g. timezone support).
</dd>
<dt>
<span class="term">
modification_time &lt; NOW()
</span>
</dt>
<dd>
This portion of the SELECT is one of the more difficult concepts to explain and therefore it is explained in detail in the next section.
</dd>
<dt>
<span class="term">
filter
</span>
</dt>
<dd>
In this section, we simply copy the value of "id" from the MySQL record into a metadata field called "_id", which we later reference in the output to ensure that each document is written into Elasticsearch with the correct "_id" value. Using a metadata field ensures that this temporary value does not cause a new field to be created. We also remove the "id", "@version", and "unix_ts_in_secs" fields from the document, as we do not wish for them to be written to Elasticsearch.
</dd>
<dt>
<span class="term">
output
</span>
</dt>
<dd>
In this section, we specify that each document should be written to the standard output using the rubydebug output to help with debugging.
</dd>
</dl>
</div>
</li>
</ul>
</div>
</li>
<li class="listitem">
<p>Launch Logstash with your new JDBC configuration file:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">bin/logstash -f jdbc.conf</pre>
</div>
<p>Logstash outputs your MySQL data through standard output <code class="literal">stdout</code>, your command line interface. The results for the initial data load should look similar to the following:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">[INFO ] 2021-04-21 12:32:32.816 [Ruby-0-Thread-15: :1] jdbc - (0.009082s) SELECT * FROM (SELECT *, UNIX_TIMESTAMP(modification_time) AS unix_ts_in_secs FROM es_table WHERE (UNIX_TIMESTAMP(modification_time) &gt; 0 AND modification_time &lt; NOW()) ORDER BY modification_time ASC) AS 't1' LIMIT 100000 OFFSET 0
{
          "client_name" =&gt; "Targaryen",
    "modification_time" =&gt; 2021-04-21T12:17:16.000Z,
           "@timestamp" =&gt; 2021-04-21T12:17:16.923Z
}
{
          "client_name" =&gt; "Lannister",
    "modification_time" =&gt; 2021-04-21T12:17:16.000Z,
           "@timestamp" =&gt; 2021-04-21T12:17:16.961Z
}
{
          "client_name" =&gt; "Stark",
    "modification_time" =&gt; 2021-04-21T12:17:16.000Z,
           "@timestamp" =&gt; 2021-04-21T12:17:16.963Z
}</pre>
</div>
<p>Logstash’s results periodically display SQL select statements, even when there’s nothing new or modified in the MySQL database:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">[INFO ] 2021-04-21 12:33:30.407 [Ruby-0-Thread-15: :1] jdbc - (0.002835s) SELECT count(*) AS 'count' FROM (SELECT *, UNIX_TIMESTAMP(modification_time) AS unix_ts_in_secs FROM es_table WHERE (UNIX_TIMESTAMP(modification_time) &gt; 1618935436 AND modification_time &lt; NOW()) ORDER BY modification_time ASC) AS 't1' LIMIT 1</pre>
</div>
</li>
<li class="listitem">
<p>Open your MySQL console. Let’s insert another record into that database using the following SQL statement:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">use es_db
INSERT INTO es_table (id, client_name)
VALUES (4,"Baratheon");</pre>
</div>
<p>Switch back to your Logstash console. Logstash detects the new record and the console displays results similar to the following:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">[INFO ] 2021-04-21 12:37:05.303 [Ruby-0-Thread-15: :1] jdbc - (0.001205s) SELECT * FROM (SELECT *, UNIX_TIMESTAMP(modification_time) AS unix_ts_in_secs FROM es_table WHERE (UNIX_TIMESTAMP(modification_time) &gt; 1618935436 AND modification_time &lt; NOW()) ORDER BY modification_time ASC) AS 't1' LIMIT 100000 OFFSET 0
{
          "client_name" =&gt; "Baratheon",
    "modification_time" =&gt; 2021-04-21T12:37:01.000Z,
           "@timestamp" =&gt; 2021-04-21T12:37:05.312Z
}</pre>
</div>
</li>
<li class="listitem">
Review the Logstash output results to make sure your data looks correct. Use <code class="literal">CTRL + C</code> to shut down Logstash.
</li>
</ol>
</div>
<h3><a id="ec-db-logstash-output"></a>Output to Elasticsearch<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/master/docs/saas/ec-getting-started-use-cases-db-logstash.asciidoc">edit</a></h3>
<p>In this section, we configure Logstash to send that MySQL data to Elasticsearch. We modify the configuration file from section <a class="xref" href="ec-getting-started-search-use-cases-db-logstash.html#ec-db-logstash-pipeline" title="Configure a Logstash pipeline with the JDBC plugin">Configure a Logstash pipeline with the JDBC plugin</a> and output it directly to Elasticsearch. We start Logstash to send the data. Afterwards, we log into Elastic Cloud and open Kibana to verify the data.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Open the <code class="literal">jdbc.conf</code> file in the Logstash folder for editing.
</li>
<li class="listitem">
<p>Update the output section with the one that follows:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">output {
  elasticsearch {
    index =&gt; "rdbms_idx"
    cloud_id =&gt; "&lt;myDeployment&gt;"
    ssl =&gt; true
    ilm_enabled =&gt; false
    # api_key =&gt; "&lt;myAPIkey&gt;"
    user =&gt; "&lt;Username&gt;"
    password =&gt; "&lt;Password&gt;"
  }
}</pre>
</div>
</li>
<li class="listitem">
<p>In the new output section, replace:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">&lt;myDeployment&gt;</code> with the Cloud ID of your Elastic Cloud deployment.
</li>
<li class="listitem">
<p>&lt;Username&gt; and &lt;Password&gt; with the credentials you use to log into Elastic Cloud. Alternatively, you may choose to use an API key to authenticate, as discussed in the next step. In this case, uncomment the <code class="literal">api_key</code> setting and leave the <code class="literal">user</code> and <code class="literal">password</code> settings commented out (<code class="literal">#</code>).</p>
<p>Following are some additional details about the configuration file settings:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
index
</span>
</dt>
<dd>
We specify the name of the Elasticsearch index, <code class="literal">rdbms_idx</code>, to associate the documents.
</dd>
<dt>
<span class="term">
cloud_id
</span>
</dt>
<dd>
Specify the ID of your Elastic Cloud deployment.
</dd>
<dt>
<span class="term">
ssl
</span>
</dt>
<dd>
Enable as Secure Socket Layer (SSL) certificates are in use with your new deployment of Elastic Cloud.
</dd>
<dt>
<span class="term">
ilm_enabled
</span>
</dt>
<dd>
Enables and disables index lifecycle management within Elasticsearch.
</dd>
<dt>
<span class="term">
api_key
</span>
</dt>
<dd>
We specify the API key created during step #4 that follows.
</dd>
</dl>
</div>
</li>
</ul>
</div>
</li>
<li class="listitem">
<p><span class="strong strong"><strong>Optional</strong></span>: For additional security, you can generate an Elasticsearch API key through the Elastic Cloud console and configure Logstash to use the new key to connect securely to the Elastic Cloud.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Log in to the <a href="https://cloud.elastic.co?baymax=docs-body&amp;elektra=docs" class="ulink" target="_top">Elasticsearch Service Console</a>.
</li>
<li class="listitem">
<p>Select your deployment on the home page in the Elasticsearch Service card or go to the deployments page.</p>
<p>Narrow your deployments by name, ID, or choose from several other filters. To customize your view, use a combination of filters, or change the format from a grid to a list.</p>
</li>
<li class="listitem">
<p>From your deployment menu, click <span class="strong strong"><strong>Elasticsearch</strong></span> &#8594; <span class="strong strong"><strong>API Console</strong></span>.</p>
<p><strong>.Select <span class="strong strong"><strong>Post</strong></span> from the drop-down list and enter <code class="literal">/_security/api_key</code> in the field.</strong>Enter the following JSON request:</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">{
 "name": "logstash-apikey",
 "role_descriptors": {
   "logstash_read_write": {
     "cluster": ["manage_index_templates", "monitor"],
     "index": [
       {
         "names": ["logstash-*","rdbms_idx"],
         "privileges": ["create_index", "write", "read", "manage"]
       }
     ]
   }
 }
}</pre>
</div>
<p>This creates an API key with the cluster <code class="literal">monitor</code> privilege which gives read-only access for determining the cluster state, and <code class="literal">manage_index_templates</code> allows all operations on index templates. Some additional privileges also allow <code class="literal">create_index</code>, <code class="literal">write</code>, and <code class="literal">manage</code> operations for the specified index. The index <code class="literal">manage</code> privilege is added to enable index refreshes.</p>
</li>
<li class="listitem">
<p>Click <span class="strong strong"><strong>Submit</strong></span>. The output should be similar to the following:</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">{
  "api_key": "tV1dnfF-GHI59ykgv4N0U3",
  "id": "2TBR42gBabmINotmvZjv",
  "name": "logstash_api_key"
}</pre>
</div>
</li>
<li class="listitem">
<p>Enter your new <code class="literal">api_key</code> value into the Logstash <code class="literal">jdbc.conf</code> file, in the format <code class="literal">&lt;id&gt;:&lt;api_key&gt;</code>. If your results were as shown in this example, you would enter <code class="literal">2TBR42gBabmINotmvZjv:tV1dnfF-GHI59ykgv4N0U3</code>. Remember to remove the pound (<code class="literal">#</code>) sign to uncomment the line, and comment out the <code class="literal">username</code> and <code class="literal">password</code> lines:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">output {
  elasticsearch {
    index =&gt; "rdbms_idx"
    cloud_id =&gt; "&lt;myDeployment&gt;"
    ssl =&gt; true
    ilm_enabled =&gt; false
    api_key =&gt; "2TBR42gBabmINotmvZjv:tV1dnfF-GHI59ykgv4N0U3"
    # user =&gt; "&lt;Username&gt;"
    # password =&gt; "&lt;Password&gt;"
  }
}</pre>
</div>
</li>
</ol>
</div>
</li>
<li class="listitem">
<p>At this point, If we simply restart Logstash as is with our new output, then no MySQL data is sent to our Elastic Cloud index.</p>
<p>Why? Logstash retains our previous <code class="literal">sql_last_value</code> timestamp and sees no new changes have occurred to our MySQL database. Therefore there’s nothing to send, as configured within our SQL statement.</p>
<p>Solution:  Adding <code class="literal">clean_run =&gt; true</code> as a new line in the jdbc input section of our <code class="literal">jdbc.conf</code> file resets the <code class="literal">sql_last_value</code> back to zero. After running it once this way, remove the <code class="literal">clean_run</code> line, unless you want this behavior to happen again at each restart of Logstash:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">input {
  jdbc {
      ...
      clean_run =&gt; true
      ...
    }
}</pre>
</div>
</li>
<li class="listitem">
<p>Go to Logstash’s installation path and open a new CLI console. Launch Logstash with your updated JDBC configuration file:</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">bin/logstash -f jdbc.conf</pre>
</div>
</li>
<li class="listitem">
<p>Logstash outputs the MySQL data to your Elastic Cloud instance. Let’s take a look in Kibana and verify that data.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Log in to the <a href="https://cloud.elastic.co?baymax=docs-body&amp;elektra=docs" class="ulink" target="_top">Elasticsearch Service Console</a>.
</li>
<li class="listitem">
<p>Select your deployment on the home page in the Elasticsearch Service card or go to the deployments page.</p>
<p>Narrow your deployments by name, ID, or choose from several other filters. To customize your view, use a combination of filters, or change the format from a grid to a list.</p>
</li>
<li class="listitem">
From your deployment menu, click <span class="strong strong"><strong>Kibana</strong></span> &#8594; <span class="strong strong"><strong>Launch</strong></span>.
</li>
<li class="listitem">
Open the side-menu panel by clicking the three vertical line button.
</li>
<li class="listitem">
Scroll down and expand Management.
</li>
<li class="listitem">
Click <span class="strong strong"><strong>Management</strong></span> &#8594; <span class="strong strong"><strong>Dev Tools</strong></span>.
</li>
<li class="listitem">
<p>Copy and paste the following API GET request into the Console pane, and then click the Play arrow. This queries all records in the new <code class="literal">rdbms_idx</code> index.</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">GET rdbms_idx/_search
{
  "query": {
    "match_all": {}
  }
}</pre>
</div>
</li>
<li class="listitem">
<p>The Results pane lists the <code class="literal">client_name</code> records originating from your MySQL database, similar to those shown below:</p>
<p><span class="image"><img src="images/ec-logstash-db-results-scenarios.png" alt="A picture showing query results with three records" width="corresponding to the Targaryen" height="Lannister"></span></p>
</li>
</ol>
</div>
</li>
</ol>
</div>
<p>Now, you should have a good understanding of how to configure Logstash to ingest data from your relational database through the JDBC plugin.  You have some design considerations to track records that are new, modified, and deleted. You should have the basics needed to begin experimenting with your own database and Elasticsearch.</p>
</div>
<div class="navfooter">
<span class="prev">
<a href="ec-getting-started-search-use-case.html">« Ingest data with Node.js on Elastic Cloud</a>
</span>
<span class="next">
<a href="ec-api-console.html">Access the Elasticsearch API console »</a>
</span>
</div>
</div>
</body>
</html>
